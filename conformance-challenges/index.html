<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
    <head>
        <meta charset="UTF-8" />

    	<title>Challenges with Accessibility Guidelines Conformance and Testing</title>
    	<script src="https://www.w3.org/Tools/respec/respec-w3c-common" class="remove"></script>
    	<script src="../biblio.js" class="remove"></script>
    	<script src="../script/wcag.js" class="remove"></script>
    	<script src="respec-config.js" class="remove"></script>
    	<link rel="stylesheet" type="text/css" href="../css/sources.css" class="remove" />
    </head>
    <body>
        <section id="abstract">
			<p>This early draft document explores situations for which the WCAG 2.x conformance model, and success criteria testability, are challenging to apply to a broad range of websites and web applications. The purpose of the document is to help understand these challenges more holistically, so that we can build better solutions in the future. For this initial draft, the challenges covered broadly fall into four main areas:</p>
			<ol>
				<li>Numerous WCAG Guidelines and Success Criteria need human involvement (with current state of technology) to test and verify conformance, which is especially challenging to scale to large sites and to dynamic sites;</li>
				<li>Large and dynamic sites have portions that are always under construction, which is challenging to address with the WCAG 2.x conformance model which makes no allowance for that;</li>
				<li>Third parties frequently add and change content on large and dynamic sites, and related non-conformities in such content make WCAG 2.x conformance significantly challenging; and</li>
				<li>Applying the WCAG 2.x conformance model is challenging to do for non-web information and communications technologies (ICT)  which it was never designed for but which it is nonetheless sometimes applied.</li>
			</ol>
			<p>While these four main areas apply to websites and web applications broadly, this early version of the document focuses particularly on situations involving large, complex, and dynamic websites.</p>
			<p>One of the reasons for publication of this document as a First Public Working Draft is to seek additional contributions describing any additional challenges, or further illustration of challenges in the existing identified areas below, ahead of work developing solutions. It is expected that a more thorough understanding of these challenges can lead to either a new conformance model, or an alternative model that is more appropriate for large, complex, and/or dynamic websites.  Ideally, such a model would also be able to distinguish between websites that are substantially accessible for most visitors with disabilities most of the time, and websites that are largely unusable by a significant portion of visitors with a disability.</p>
      <p>This document also includes previously published research from the Silver Task Force and Community Group that was specifically related to Challenges with Conformance.</p>
		</section>
        <section id="sotd">
            <p>To comment, <a href="https://github.com/w3c/wcag/issues/new">file an issue in the <abbr title="World Wide Web Consortium">W3C</abbr> WCAG GitHub repository</a>. Please associate your issue with the "Challenges with Conformance" WCAG label. Although the proposed Success Criteria in this document reference issues tracking discussion, the Working Group requests that public comments be filed as new issues, one issue per discrete comment. It is free to create a GitHub account to file issues. If filing issues in GitHub is not feasible, send email to <a href="mailto:public-agwg-comments@w3.org?subject=WCAG%202.2%20public%20comment">public-agwg-comments@w3.org</a> (<a href="https://lists.w3.org/Archives/Public/public-agwg-comments/">comment archive</a>). </p>
        </section>
        <section class="introductory">
			<h2>Introduction</h2>
			<p>Unchanged since WCAG 2.0 was published in December 2008, the Web Content Accessibility Guidelines list a <a href="https://www.w3.org/TR/WCAG20/#conformance">set normative of requirements “in order for a web page to conform to” WCAG 2.x</a>, including setting forth that conformance “is for full Web page(s) only, and cannot be achieved if part of a Web page is excluded”, along with a Note that states “If a page cannot conform (for example, a conformance test page or an example page), it cannot be included in the scope of conformance or in a conformance claim.”  Also unchanged is text setting forth what is allowed in any optional “Conformance Claims”, starting with text that states: “Conformance is defined only for <a href="https://www.w3.org/TR/WCAG20/#webpagedef">Web pages</a>. However, a conformance claim may be made to cover one page, a series of pages, or multiple related Web pages.”</p>
			<p>For the purposes of this document, we use the term “WCAG 2.x conformance model” to refer to the normative text in the <a href="https://www.w3.org/TR/WCAG20/#conformance">Conformance section</a> of WCAG 2.0 and WCAG 2.1.  We also note that this model was expressly defined for individual full web pages of a website, or for a series of web pages when they represent a process.  We further note that a conformance claim is specifically defined as something that covers “one page, a series of pages, or multiple related Web pages.” These terms, “a series of” and “multiple” are commonly understood in English to mean a fairly small number of items – more than one (e.g. “several”), but not many thousands or millions.</p>
			<p class="note">Language like “A Series of Unfortunate Events” “she published a series of books”, or “I subscribed to the concert series”; and language like “there were multiple births” or “there are multiple owners” all are broadly understood as not numbering in many thousands or millions.</p>
			<p>Large websites are often highly complex, with substantial dynamic content, including content updates, new content, and user interface changes that happen almost continuously, perhaps at the rate of hundreds or even thousands of page updates per second. This is especially the case where third parties are actively populating and changing site content. Fundamentally, these large sites are never finished. Some portion is always changing. In such situations, we should anticipate that portions of a web site will be incomplete or evolving for various reasons at any moment.  Thus, the likelihood that every last page (out of what might be millions or billions of pages) can satisfy each and every one of the WCAG success criteria 100% of the time is so low as to be impossible to expect. </p>
			<p>The goal of this work is not the avoidance of such situations, because they are as unavoidable on the web as in the physical world. The goal is the eventual establishment of reasonable consensus on expectations that maximize accessibility, and limit service downtime, in the on-line environment. We believe that a better understanding of the situations in which WCAG 2.x conformance applies poorly if at all, can lead to more effective models for large, complex, and dynamic websites. </p>
        </section>
		<section>
			<h2>Challenge #1: Specific WCAG Guidelines &amp; Success Criteria needing Human Involvement</h2>
			<p>A challenge common to many of the WCAG Guidelines and Success Criteria is the inability for automatic testing to fully validate conformance to WCAG 2.x and the
			   subsequent time, cost, and expertise needed to perform the necessary manual test to cover the full range of the requirements.  The HTML markup designed for supporting accessibility can be automatically validated. What is challenging to be verified automatically is whether the criteria have been helpfully used, or whether the content they present to the user is appropriate. For large and/or dynamic web sites, perhaps numbering in the hundreds of thousands of pages or more, and for sites being updated hundreds or thousands of times per second or more, so many existing accessibility success criteria expect informed human evaluation that it is impossible for humans to validate every page and every update. The same can be said of
			   very large web-based applications that are developed in an agile manner with updates delivered in rapid succession, often on an hourly basis.</p>
			<p>We can think of this as the distinction between quantitative and qualitative analysis. We know how to automatically test for and count the occurrences of relevant markup. However, we do not yet know how to automatically verify the quality of what that markup conveys to the user. In the case of adjudging appropriate quality, informed human review is still required.</p>
			<p>This (growing) section describes challenges with applying the WCAG 2.x conformance model to specific Guidelines and Success Criteria, primarily based on required human involvement in evaluation of conformance to them. In this draft, the list is not  exhaustive, but we intend it to cover all known challenges with at least all A and AA Success Criteria, by the time this Note is completed.</p>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#text-alternatives">Text Alternatives (Guideline 1.1) for Non-Text Content (Success Criterion 1.1)</a></h3>
			<p>Text alternatives for images are an early, and still widely used, accessibility enhancement to HTML. Yet text alternatives remain one of the more intractable success criteria to assess with automated accessibility checking. While testing for the presence of alternative text is straightforward, and a collection of specific errors (such as labeling a graphic "spacer.gif") can be identified by automated testing, human judgment remains necessary to evaluate whether or not any particular text alternative for a graphic is correct and conveys the true meaning of the image.  Image recognition techniques are not mature enough to fully discern the underlying meaning of an image and the intent of the author in its inclusion. As a simple example, an image or icon of a paper clip would likely be identified by image recognition simply as a "paper clip." However, when a paper clip appears in content often its meaning is to show there is an attachment. In this specific example, the alternative text should be "attachment," not "paper clip." Similarly, the image of a globe (or any graphical object representing planet Earth) can be used for a multiplicity of reasons, and the appropriate alternative text should indicate the reason for that use and not descriptive wording such as "globe" or "planet Earth." One not uncommon use of a globe today expands to allow users to select their preferred language, but there may be many other reasonable uses of such an icon.</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#time-based-media">Time-Based Media (Guideline 1.2)</a></h3>
			<p>Practices for creating alternatives to spoken dialog, and to describe visual content, were established in motion picture and TV content well before the world wide web came into existence. These practices formed the basis of the <a href="http://www.w3.org/TR/media-accessibility-reqs/">Media Accessibility User Requirements (MAUR)</a> for time-based streaming media on the web in HTML5, which now supports both captioning and descriptions of video.</p>
			<p>Yet, just as with text alternatives, automated techniques and testing aren’t sufficient for creating and validating accessible alternatives to time-based media.  For example, Automatic Speech Recognition (ASR) often fails when the speech portion of the audio is low quality, isn’t clear, or has background noise or
			sound-effects.  In addition, current automated transcript creation
			   software doesn't perform speaker identification, meaningful sound identification, or correct punctuation that all are necessary for accurate captioning. Work on automatically generated descriptions of video are in their infancy, and like image recognition techniques, don’t provide usable alternatives to video.</p>
			<p>Similarly, while there is well articulated guidance on how to create text transcripts or captions for audio-only media (such as radio programs and audio books), automated techniques and testing again aren’t sufficient for creating and validating these accessible alternatives.  Knowing what is important in an audio program to describe to someone who cannot hear is beyond the state of the art. There are several success criteria under this Guideline that all share these challenges of manual testing being required to ensure alternatives accurately reflect the content in the media.  These include:</p>
			<ul>
				<li>Audio-only and video-only (Prerecorded) (Success Criterion 1.2.1)</li>
				<li>Captions (Prerecorded) (Success Criterion 1.2.2)</li>
			<li>Audio description or media alternative (Prerecorded) (Success Criterion 1.2.3)</li>
				<li>Captions (Live) (Success Criterion 1.2.4)</li>
			<li>Audio description (Prerecorded) (Success Criterion 1.2.5)</li>
			</ul>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#info-and-relationships">Info and Relationships (Success Criterion 1.3.1)</a></h3>
			<p>Whether in print or online, the presentation of content is often structured in a manner intended to aid comprehension. Sighted users perceive structure and relationships through various visual cues. Beyond simple sentences and paragraphs, the sighted user may see headings with nested subheadings. There may be sidebars and inset boxes of related content. Tables may be used to show data relationships. Comprehending how content is organized is a critical component of understanding the content.</p>
			<p>As with media above, automated testing can determine the presence of structural markup, and can flag certain visual presentations as likely needing that structural markup. But such automated techniques remain unable to decipher if that markup usefully organizes the page content in a way that a user relying on assistive technology can examine the page systematically and readily understand its content.</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#meaningful-sequence">Meaningful Sequence (Success Criterion 1.3.2)</a></h3>
			<p>Often the sequence in which content is presented affects its meaning. In some content there may be even more than one meaningful way of ordering that content. However, as with Info and Relationships above, automated techniques are unable to determine whether read aloud users will be able to read the content in a meaningful sequence ordering.</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#sensory-characteristics">Sensory Characteristics (Success Criterion 1.3.3)</a></h3>
			<p>Ensuring that no instructions rely on references to sensory characteristics presents similar challenges to ensuring that color isn’t the sole indicator of meaning (<a href="https://www.w3.org/TR/WCAG21/#use-of-color">Success Criterion 1.4.1</a>) – it is testing for a negative, and requires a deep understanding of meaning conveyed by the text to discern a failure programmatically.  For example, while instructions such as “select the red button” reference a sensory characteristic, “select the red button which is also the first button on the screen” may provide sufficient non-sensory context to not cause a problem (and multi-modal, multi-sensory guidance is often better for users with cognitive impairments or non-typical learning styles).</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#orientation">Orientation (Success Criterion 1.3.4)</a></h3>
			<p>While an automated test can determine that the orientation is locked, full evaluation of conformance to this criterion is tied to whether it is “<a href="https://www.w3.org/TR/WCAG21/#dfn-essential">essential</a>” for the content to be locked to one specific orientation (e.g. portrait or landscape views of an interface rendered to a cell phone).  This requires human judgement to ensure that, any time the orientation is locked, the
			   orientation is essential to that content, to determine
			   conformance.  This makes this criterion not fully automatable.</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#identify-input-purpose">Identify Input Purpose (Success Criterion 1.3.5)</a></h3>
			<p>An automated test can easily determine that input fields use HTML markup to indicate the input purpose, it requires manual verification to determine that the correct markup was used that matches the intent for
			   the field. For example, for a name input field, there are 10 variations of HTML name purpose attributes with different meaning and using the incorrect markup would be confusing to the user.</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#use-of-color">Use of Color (Success Criterion 1.4.1)</a></h3>
			<p>This poses the same challenges as Sensory Characteristics (<a href="https://www.w3.org/TR/WCAG21/#sensory-characteristics">Success Criterion 1.3.3</a>).  To discern whether a page fails this criterion programmatically requires understanding the full meaning of the related content on the page and whether any meaning conveyed by color is somehow also conveyed in another fashion (e.g. whether the meaning of the colors in a bar chart is conveyed in the body of associated text or with a striping/stippling pattern as well on the bars, or perhaps some other fashion).</p>
		</section>

		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#audio-control">Audio Control[36] (Success Criterion 1.4.2)</a></h3>
			<p>An automated test tool would be able to identify media/audio content in a website, identify whether auto-play is turned on in the code, and also determine the duration.  However, an automated checking tool
			cannot tell whether there is a mechanism to pause or stop the audio or adjust the volume of the audio independent of the overall system volume level. This would have to be manually checked.</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#contrast-minimum">Contrast (Minimum) (Success Criterion 1.4.3)</a></h3>
			<p>Automated tools can check the color of text against the background in most cases. However, when background images are used, automated tests aren't typically able to check for minimum contrast of text against
      the image&mdash;especially if the image is a photograph or drawing where the text is placed over the image. This would take human intervention to sample the text and its background to determine if the contrast meets the
      minimum requirement.</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#resize-text">Resize Text (Success Criterion 1.4.4)</a></h3>
			<p>It takes human intervention to discern visually if content is overlapped or missing due to a change in text size.</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#images-of-text">Images of Text (Success Criterion 1.4.5)</a></h3>
			<p>This poses the same challenge as Orientation (<a href="https://www.w3.org/TR/WCAG21/#orientation">Success Criterion 1.3.4</a>) - it is tied to whether it is “<a href="https://www.w3.org/TR/WCAG21/#dfn-essential">essential</a>” for text to be part of an image.  This requires human judgement, making this criterion not readily automatable. In addition, methods of employing OCR on images will not accurately discern text of different fonts that overlap each other or be able to recognize unusual characters or text that is of poor
			   contrast with the background of the image.</p>
		</section>
		<section>
		<h3><a href="https://www.w3.org/TR/WCAG21/#reflow">Reflow[43] (Success Criterion 1.4.10)</a></h3>
		<p>Human intervention is needed to ensure there is not both horizontal and vertical scrolling (only one dimension of scrolling) as well as make the determination for when two-dimensional scrolling is needed for
		content that requires two-dimensional layout for usage or meaning.</p>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#non-text-contrast">Non-text Contrast (Success Criterion 1.4.11)</a></h3>
			<p>This success criterion needs several levels of checks that are either difficult or impossible to automate. This is especially true when there are exceptions in criteria such as this one which would require
			human intervention to examine the intent and potentially employ the exception to comply with the requirement. The automated checks would have to include:</p>
			<ul>
				<li>A way to determine what are the UI components, easy for standard HTML elements, but more difficult for non-standard custom scripted components.</li>
   <li>Whether the default user agent visual treatments for identifying the UI components and states are being used (so that the exception can be utilized)</li>
   <li>Where default treatments are not employed, a way to identify changes in state and then compare the two states for sufficient contrast. Typically this would require human intervention to test the differences in the portions of graphics used to show the different states or provides the meaning (e.g. checked, unchecked, radio button selected, radio button unselected, toggle button selected vs. unselected and so on).</li>
   <li>For graphical objects, a way to identify what part of the graphics are required to understand the content. Once identified, checks to determine if the presentation of the graphics is ■~@~\essential■~@~] to utilize the exception - a part that certainly would require human intervention.</li>
			</ul>
		</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#text-spacing">Text Spacing[45] (Success Criterion 1.4.12)</a></h3>
			<p>This success criterion involves using a tool or method to modify the text spacing and then checking to ensure no content is truncated or overlapping. This is purely a manual process.</p>
		</section>
<section>
<h3>Content on Hover or Focus (Success Criterion 1.4.13)</h3>
<p>As content needs to be brought up by hovering the pointer over it (or
keyboard-focus it) to then determine whether the 3 criteria of 1.4.13 are met,
this is by necessity a human test.</p>
</section>
		<section>
			<h3><a href="https://www.w3.org/TR/WCAG21/#keyboard">Keyboard Operable[47] (Success Criterion 2.1.1)</a></h3>
			<p>To ensure keyboard operability of all functionality, it requires a very manual process of navigating through the content to ensure all interactive elements are in the tab order and can be fully operated using
			the keyboard.</p>
		</section>
<section>
<h3>Character Key Shortcuts (Success Criterion 2.1.4)</h3>
<p>single key presses can be applied to content via a script but whether and
what these keypresses trigger can only determined by additional human
checks.</p>
</section>
<section>
<h3>Timing Adjustable (Success Criterion 2.2.1)</h3>
<p>There is no easy way to check automatically whether timing is adjustable. Ways of controlling differ both in naming, position and approach (including dialogs/popups before the time-out) and depend on the way the
   server registers user interactions (e.g. for automatically extending the
time-out).</p>
</section>
<section>
<h3>Pause, Stop, Hide (Success Criterion 2.2.2)</h3>
  <p>Typically the requirement to control moving content id provided by some interactive controls placed in the vicinity of moving content, or rarely on a general level at the beginning of content. The fact that position
   and naming vary means that an assessment is mostly human task (it will
involve checking that the function works as expected).</p>
</section>
<section>
<h3>Three Flashes or Below Threshold (Success Criterion 2.3.1)</h3>
<p>May be automatable but it involved an assessment of the area of flashing,
so probably human. As this Failure hardly ever occurs, I just mention it for
completeness.</p>
</section>
<section>
<h3>Bypass Blocks (Success Criterion 2.4.1)</h3>
<p>You can determine whether native elements or landmark roles are used but probably not if they are used to adequately structure content (are they missing out on sections that should be included). The same assessment
   would be needed when other Techniques are used (structure by headings, skip
links).</p>
</section>
<section>
<h3>Page titled (Success Criterion 2.4.2)</h3>
<p>Easy to check automatically whether the page has a title but the
descriptive check is a human one. Matching title to h1 will not cover pass
cases where title and h1 differ.</p>
</section>
<section>
<h3>Focus Order (Success Criterion 2.4.3)</h3>
<p>Focus handling with dynamic content (move content to custom dialog, keep
focus in dialog, return to trigger) do not look like they will be automatable
any time soon.</p>
</section>
<section>
<h3>Link Purpose (Success Criterion 2.4.4)</h3>
<p>Automated can check for the existence of same named links and probably check whether they are qualified programmatically, but checking whether the context adequately serves to describe link purpose seems like still
   involving human judgment.</p>
</section>
<section>
<h3>Multiple ways (Success Criterion 2.4.5)</h3>
<p>Automatically checking the presence of several ways (e.g. nav and search) helps but may miss cases where exceptions hold (all pages can be reached from anywhere) so needs a human check? But maybe algorithms already
   exist to check that.</p>
</section>
<section>
<h3>Headings and Labels (Success Criterion 2.4.6)</h3>
<p>The determination whether headings are descriptive depend on an assessment
of the context of web content headed or labeled and is therefore
predominantly a human assessment.</p>
</section>
<section>
<h3>Pointer Gestures (Success Criterion 2.5.1)</h3>
<p>I am not aware of an automated check that would detect complex gestures -
even when a script indicates the presence of particular events like touch-start,
the event called would need to be checked in human evaluation.</p>
</section>
<section>
<h3>Pointer Cancellation (Success Criterion 2.5.2)</h3>
<p>When mouse-down events are used (this can be done automatically), checking
for one of the four options that make it OK looks definitely like a human
task.</p>
</section>
<section>
<h3>Motion Actuation (Success Criterion 2.5.4)</h3>
<p>Event suspect may be detected automatically but whether there are
equivalents for achieving the same thing via user interface components will
require a human check.</p>
</section>
<section>
<h3>On Focus (Success Criterion 3.2.1)</h3>
<p>I suspect it is hard to check automatically whether a change caused by
focusing should be considered a change of content or context.</p>
</section>
<section>
<h3>On Input (Success Criterion 3.2.2)</h3>
<p>I suspect it is hard to check automatically whether a change caused by inputting stuff should be considered a change of content or context, or to automatically detect whether relevant advice exist before the
   component in question.</p>
</section>
<section>
<h3>Error Identification (Success Criterion 3.3.1)</h3>
<p>Whether the error message correctly identifies and describes the error will
often involve human judgment.</p>
</section>
<section>
<h3>Labels or Instructions (Success Criterion 3.3.2)</h3>
<p>Edge cases (are labels close enough to a component to be perceived as a visible label) will need human check. Some label may be programmatically linked but hidden or far off. Whether instructions are necessary and
   need to be provided will hinge of the content. Human check needed.</p>
</section>
<section>
<h3>Error Suggestion (Success Criterion 3.3.3)</h3>
<p>Whether a suggestion is helpful / correct will often involve human
judgment.</p>
</section>
<section>
<h3>Name, Role, Value (Success Criterion 4.1.2)</h3>
<p>Incorrect use of ARIA constructs can be detected automatically but constructs that appear correct may still not work, and widgets that have NO Aria (but would need it to be understood) can go undetected. Human
   post-check of automatic checks seems necessary.</p>
</section>
		</section>
    	<section>
				<h2>Challenge #2: Large, complex, and dynamic websites are always “under construction”</h2>
				<p>Large websites often have complex content publishing pipelines, which may render content dynamically depending upon a large number of variables (such as what is known about the logged in user and her content preferences, the geographical location that the user is visiting the site from, and the capabilities of the web rendering agent being used).  It may not be possible to validate every possible publishing permutation, each of which can have an impact on whether that particular rendering of the content at that particular moment conforms under the WCAG 2.x conformance model. </p>
		</section>
		<section>
				<h2>Challenge #3: 3rd party content</h2>
				<p>Very large, highly dynamic web sites generally aggregate content provided by multiple entities. Many of these are third parties with the ability to add content directly to the website – including potentially every website visitor. While the website can provide guidance on how to post content so that it meets WCAG Guidelines and Success Criteria, it is ultimately up to those third parties to understand and correctly implement that guidance.  And as noted above, even with automated checking prior to accepting the post, too many Guidelines and Success Criteria expect human validation involvement.</p>
				<p>Copyright and similar constraints that restrict the ability to modify or impose requirements on third party data can also make full page conformance impossible to assure. For instance: articles that allow reposting
				   but without modification due to copyright restrictions.</p>
		</section>
		<section>
				<h2>Challenge #4: Non-Web Information and Communications Technologies </h2>
				<p>The core principles, and many of the guidelines, contained in WCAG 2.x, are broadly applicable outside of the web context.  For example, no matter the technology, information presented to humans needs to be perceivable by them in order for them to access and use it.  At the same time, some of the specific guidelines and especially some of the individual success criteria of WCAG 2.x are written specifically for web content and web technologies, and may be difficult to apply to non-web information and communications technologies (as set forth in the Working Group Note “<a href="https://www.w3.org/TR/wcag2ict/">Applying WCAG to Non-Web Information and Communication Technologies</a>” or “WCAG2ICT”).  A number of entities looking to apply WCAG 2.x to non-web ICT have made use of the guidance in WCAG2ICT when drafting regulations and requirements in their
					contexts and countries.</p>
				<p>Where non-web ICT share similar characteristics of size, complexity, and dynamism as large, complex, and dynamic websites, &lt;&lt;FINISH ME>></p>
				<p>Some of the specific Success Criteria pose specific challenges in the WCAG2ICT context.</p>
				<section>
					<h3><a href="https://www.w3.org/TR/WCAG21/#identify-input-purpose">Identify Input Purpose (Success Criterion 1.3.5)</a></h3>
					<p>This Success Criterion specifies that an input field’s purpose must be “identified in the <a href="https://www.w3.org/TR/WCAG21/#input-purposes">Input Purposes for User Interface Components section</a>” of WCAG 2.x.  Unfortunately, few if any operating systems or platforms have the same or an equivalent definition of input components that can be conveyed to assistive technologies [add reference to Yet Another Accessibility API Mapping project here].</p>
				</section>
		</section>
		<section>
		<h2>Challenges of Conformance as identified from Silver Research</h2>
		<p>The Silver Accessibility Guidelines project was designed to be research-based. The Silver Task Force of the Accessibility Guidelines Working Group and the Silver Community Group collaborated with researchers on questions that the Silver Group identified. This research was used to develop 11 problem statements that needed to be solved for Silver.   The detailed problem statements include the specific problem, the result of the problem, the situation and priority, and the opportunity presented by the problem.  The problem statements were organized into three main areas:  Usability, Conformance, Maintenance. The section following is taken from the <a href="https://www.w3.org/community/silver/draft-final-report-of-silver/#h.n2wcg0gauz5x">Conformance sections of the Silver Design Sprint Final Report</a> and the <a href="https://www.w3.org/WAI/GL/task-forces/silver/wiki/Problem_Statements#Conformance_Model">Silver Problem Statements</a>. Details of the research questions and the individual reports are in <a href="https://www.w3.org/WAI/GL/task-forces/silver/wiki/Silver_Research_Archive">Research Archive of Silver wiki</a>. </p>
			<section>
			<h3>Silver Research Problem Statements</h3>
			<p>Originally published as the <a href="https://www.w3.org/community/silver/draft-final-report-of-silver/#h.n2wcg0gauz5x">Silver Design Sprint Final Report</a> (2018).  These problem statements were presented to the Silver Design Sprint participants.</p>
			<ul>
			<li>Constraints on “What is Strictly Testable” provides an obstacle to including guidance that meets the needs of people with disabilities but is not conducive to a pass/fail test. </li>
			<li>Human Testable (related to Ambiguity) also relates to differences in knowledge and priorities of different testers achieve different results.</li>
			<li>Accessibility Supported is a conformance requirement of WCAG 2  that is poorly understood and incompletely implemented</li>
			<li>Evolving Technology of the rapidly changing web must constantly be evaluated against the capabilities of assistive technology and evolving assistive technology must be evaluated against the backward compatibility of existing web sites. </li>
			</ul>
			</section>
			<section>
			<h3>Details of Problem Statements</h3>
			<p>Originally published as <a href="https://www.w3.org/WAI/GL/task-forces/silver/wiki/Problem_Statements#Conformance_Model">Silver Problem Statements</a>, this was a detailed analysis of the research results behind the above list.  
				<section>
				<h4>Definition of Conformance</h4>
<p>“Conformance to a standard means that you meet or satisfy the ‘requirements’ of the standard. In WCAG 2.0 the ‘requirements’ are the Success Criteria. To conform to WCAG 2.0, you need to satisfy the Success Criteria, that is, there is no content which violates the Success Criteria.”</p>
<p><strong>WCAG 2.0 Conformance Requirements:</strong></p>
<ol>
<li>Conformance Level (A to AAA)
<li>Conformance Scope (For full web pages only, not partial)
<li>Complete Process
<li>Only “Accessibility-supported” ways of using technologies
<li>Non-Interference: Technologies that are not accessibility supported can be used, as long as all the information is also available using technologies that are accessibility supported and as long as the non-accessibility-supported material does not interfere.
</ol>
				</section>
				<section>
				<h4>Themes from Research</h4>
				<ul>
<li>No monitoring process to test the accuracy of WCAG compliance claims (Keith et al, 2012)
<li>Difficulties for conformance (Keith et al, 2012)
	<ul>
<li>Third parties documents, applications and services
<li>Know-how of IT personnel
<li>Tension between accessibility and design
				</ul>
<li>Specific success criteria for failure - 1.1.1 , 2.2., 4.1.2 (Keith et al, 2012)
<li>“Reliably Human Testable” , “not reliably testable” (Brajnick et al, 2012) average agreement was at the 70–75% mark, while the error rate was around 29%.
				<ul>
<li>Expertise appears to improve (by 19%) the ability to avoid false positives. Finally, pooling the results of two independent experienced evaluators would be the best option, capturing at most 76% of the true problems and producing only 24% of false positives. Any other independent combination of audits would achieve worse results.
<li>This means that an 80% target for agreement, when audits are conducted without communication between evaluators, is not attainable, even with experienced evaluators 
				</ul>
<li>Challenges and Recommendations (Alonso et al, 2010)
				<ul>
<li>“accessibility supported ways of using technologies”.
<li>Testability of Success Criteria
<li>Openness of Techniques and Failures
<li>Aggregation of Partial Results
				</ul></li>
<li>Silver need to expand the scope beyond web to include apps, documents, authoring, user agents, wearables, kiosks, IoT, VR, etc. and be inclusive of more disabilities. (UX Professionals Use of WCAG: Analysis)
<li>Accessibility Supported allows inadequate assistive technologies to be claimed for conformance, particularly in non-native English speaking countries. (Interviews on Conformance)
				</ul>
				</section>
				<section>
				<h4>Constraints on What is Strictly Testable</h4>
<p><strong>Specific problem</strong>: Certain success criteria are quite clear and measurable, like color contrast. Others, far less so. The entire principle of understandable is critical for people with cognitive disabilities, yet success criteria intended to support the principle are not easy to test for or clear on how to measure. As a simple example, there is no clear, recent or consistent definition – within any locale or language – on what “lower secondary education level” means in regards to web content. Language and text content is also not the only challenge among those with cognitive and learning disabilities. Compounding this, most of the existing criteria in support of understanding are designated as AAA, which relatively few organizations attempt to conform with.
</p>
<p>
<strong>Result of problem</strong>: The requirement for strict testability for WCAG success criteria presents a structural barrier to including the needs of people with disabilities whose needs are not strictly testable. Guidance that WCAG working group members would like to include cannot be included. The needs of people with disabilities – especially intellectual and cognitive disabilities – are not being met.
</p>
<p>
<strong>Situation and Priority</strong>: Of the 70 new success criteria proposed by the Cognitive Accessibility Task Force to support the needs of people with cognitive and intellectual disabilities, only four to six (depending on interpretation) were added to WCAG 2.1 and only one is in level AA. The remainder are in level AAA, which is rarely implemented. There is a failure of expectations and a failure to meet user needs.
</p>
<p>
<strong>Opportunity</strong>: Multiple research projects and audience feedback have concluded that simpler language is desired and needed for audiences of the guidelines. Clear but flexible criteria with considerations for a wider spectrum of disabilities helps ensure more needs are met.
</p>
			</section>
			<section>
			<h4>Human Testable</h4>
<p>
<strong>Specific problem</strong>: Regardless of proficiency, there is a significant gap in how any two human auditors will identify a success or fail of criteria. Various audiences have competing priorities when assessing the success criteria of any given digital property. Knowledge varies for accessibility standards and how people with disabilities use assistive technology tools. Ultimately, there is variance between: any 2 auditors; any 2 authors of test cases; and human bias. Some needs of people of disabilities are difficult to measure in a quantifiable way.
</p>
<p>
<strong>Result of problem</strong>: Success criteria are measured by different standards and by people who often make subjective observations. Because there’s so much room for human error, an individual may believe they’ve met a specific conformance model when, in reality, that’s not the case. The ultimate impact is on an end user with a disability who cannot complete a given task, because the success criteria wasn’t properly identified, tested and understood.
</p>
<p>
<strong>Situation and Priority</strong>: There isn’t a standardized approach to how the conformance model applies to success criteria at the organizational level and in specific test case scenarios.
</p>
<p>
<strong>Opportunity</strong>: There’s an opportunity to make the success criteria more clear for human auditors and testers. Educating business leaders on how the varying levels of conformance apply to their organization may be useful as well. We can educate about the ways that people with disabilities use their assistive technology.
</p>
			</section>
			<section>
<h4>Accessibility Supported</h4>
<p>
<strong>Specific problem</strong>: ‘Accessibility supported’ was never fully implemented in a way that was clear and useful to developers and testers. It also requires a harmonious relationship and persistent interoperability between content technologies and requesting technologies that must be continuously evaluated as either is updated. Further, the WG “defers the judgment of how much, how many, or which AT must support a technology to the community”. It is poorly understood, even by experts.
</p>
<p>
<strong>Result of problem</strong>: Among the results are: difficulty understanding what qualifies as a content technology or an assistive technology; difficulty quantifying assistive technologies or features of user agents; claiming conformance with inadequate assistive technology; and difficulty claiming conformance.
</p>
<p>
<strong>Situation and Priority</strong>: Any claim or assertion that a web page conforms to the guidelines may require an explicit statement defining which assistive technology and user agent(s) the contained technologies rely upon, and presumably inclusive of specific versions and or release dates of each. One could infer then that a conformance claim is dependant upon a software compatibility claim naming browsers and assistive technology and their respective versions. This would create a burden to author and govern such claims. Additionally, no one can predict and anticipate new technologies and their rates of adoption by people with disabilities.
</p>
<p>
<strong>Opportunity</strong>: As the technologies in this equation evolve, the interoperability may be affected by any number of factors outside of the control of the author and publisher of a web page. Either “accessibility supported” should be removed from conformance requirements, or it should clearly, concisely and explicitly define and quantify the technologies or classes of technologies, AND set any resulting update or expiry criteria for governance.
</p>
				</section>
				<section>
<h4>Evolving Technology</h4>
<p>
<strong>Specific problem</strong>: Evolving Technology: As content technology evolves, it must be re-evaluated against assistive technology for compatibility. Likewise, as assistive technology evolves or emerges, it must be evaluated against the backward compatibility of various content technology.
</p>
<p>
<strong>Result of problem</strong>: There is no versioning consideration for updates to user agents and assistive technology. Strict conformance then typically has an expiry.
</p>
<p>
<strong>Situation and Priority</strong>: There is no clear and universal understanding of the conformance model or its longevity. Some will infer that there is always a conformance debt when any technology changes.
</p>
<p>
<strong>Opportunity</strong>: Consider conformance statements to include a explicit qualifier of time of release or versions of technology. OR consider a more general approach that is not explicit and is flexible to the differences in technologies as they evolve, identifying the feature of the assistive tech rather than the version of the assistive tech. OR consider a model that quantifies conformance as a degree of criteria met.
</p>
			</section>

			</section>
		</section>
        <section>
            <h1>Terms</h1>
        </section>
    	    	
      <div data-include="../acknowledgements.html" data-include-replace="true">
            <p>Acknowledgements</p>
        </div>
    </body>
</html>
